<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Getting Started with StarCluster &mdash; StarCluster v0.91 documentation</title>
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '#',
        VERSION:     '0.91',
        COLLAPSE_MODINDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="StarCluster v0.91 documentation" href="index.html" />
    <link rel="next" title="Creating a New AMI From the StarCluster AMI" href="create_new_ami.html" />
    <link rel="prev" title="Launching a StarCluster on Amazon EC2" href="launch.html" /> 
  </head>
  <body>

<div style="background-color: white; text-align: left; padding: 10px 10px 15px 15px">
<a href="index.html"><img src="_static/logo.png" border="0" alt="StarCluster"/></a>
</div>

    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="create_new_ami.html" title="Creating a New AMI From the StarCluster AMI"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="launch.html" title="Launching a StarCluster on Amazon EC2"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">home</a>|&nbsp;</li>
        <li><a href="search.html">search</a>|&nbsp;</li>
       <li><a href="contents.html">documentation </a> &raquo;</li>
 
      </ul>
    </div>

      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <h3><a href="index.html">Table Of Contents</a></h3>
            <ul>
<li><a class="reference external" href="#">Getting Started with StarCluster</a><ul>
<li><a class="reference external" href="#logging-into-the-master-node">Logging into the master node</a></li>
<li><a class="reference external" href="#logging-into-a-worker-node">Logging into a worker node</a></li>
<li><a class="reference external" href="#verify-etc-hosts">Verify /etc/hosts</a></li>
<li><a class="reference external" href="#verify-passwordless-ssh">Verify Passwordless SSH</a></li>
<li><a class="reference external" href="#verify-home-is-nfs-shared">Verify /home is NFS Shared</a></li>
<li><a class="reference external" href="#ensure-ebs-volumes-are-mounted-and-nfs-shared-optional">Ensure EBS Volumes are Mounted and NFS shared (OPTIONAL)</a></li>
<li><a class="reference external" href="#verify-scratch-space">Verify scratch space</a></li>
<li><a class="reference external" href="#compile-and-run-a-hello-world-openmpi-program">Compile and run a &#8220;Hello World&#8221; OpenMPI program</a></li>
<li><a class="reference external" href="#sun-grid-engine-sge-quickstart">Sun Grid Engine (SGE) QuickStart</a></li>
<li><a class="reference external" href="#monitoring-jobs-in-the-queue">Monitoring Jobs in the Queue</a></li>
<li><a class="reference external" href="#viewing-a-job-s-output">Viewing a Job&#8217;s Output</a></li>
<li><a class="reference external" href="#monitoring-cluster-usage">Monitoring Cluster Usage</a></li>
<li><a class="reference external" href="#creating-a-job-script">Creating a Job Script</a></li>
<li><a class="reference external" href="#deleting-a-job-from-the-queue">Deleting a Job from the Queue</a></li>
</ul>
</li>
</ul>

            <h4>Previous topic</h4>
            <p class="topless"><a href="launch.html"
                                  title="previous chapter">Launching a StarCluster on Amazon EC2</a></p>
            <h4>Next topic</h4>
            <p class="topless"><a href="create_new_ami.html"
                                  title="next chapter">Creating a New AMI From the StarCluster AMI</a></p>
            <h3>This Page</h3>
            <ul class="this-page-menu">
              <li><a href="_sources/getting_started.txt"
                     rel="nofollow">Show Source</a></li>
            </ul>
          <div id="searchbox" style="display: none">
            <h3>Quick search</h3>
              <form class="search" action="search.html" method="get">
                <input type="text" name="q" size="18" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
              </form>
              <p class="searchtip" style="font-size: 90%">
              Enter search terms or a module, class or function name.
              </p>
          </div>
          <script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="getting-started-with-starcluster">
<h1>Getting Started with StarCluster<a class="headerlink" href="#getting-started-with-starcluster" title="Permalink to this headline">¶</a></h1>
<p>After you&#8217;ve created a StarCluster on Amazon, it&#8217;s time to login and do some real work.
The sections below explain how to access the cluster, verify that everything&#8217;s configured
properly, and how to use OpenMPI and Sun Grid Engine on StarCluster.</p>
<p>For these sections we used a small two node StarCluster for demonstration. In some cases, you
may need to adjust the instructions/commands for your size cluster. For all sections, we assume
<strong>cluster_user</strong> is set to <em>sgeadmin</em>.  If you&#8217;ve specified a different <strong>cluster_user</strong>, please
replace sgeadmin with your <strong>cluster_user</strong> in the following sections.</p>
<div class="section" id="logging-into-the-master-node">
<h2>Logging into the master node<a class="headerlink" href="#logging-into-the-master-node" title="Permalink to this headline">¶</a></h2>
<p>To login to the master node as root:</p>
<div class="highlight-none"><div class="highlight"><pre>$ starcluster sshmaster mycluster
StarCluster - (http://web.mit.edu/starcluster)
Software Tools for Academics and Researchers (STAR)
Please submit bug reports to starcluster@mit.edu

The authenticity of host &#39;ec2-123-123-123-231.compute-1.amazonaws.com (123.123.123.231)&#39; can&#39;t be established.
RSA key fingerprint is 85:23:b0:7e:23:c8:d1:02:4f:ba:22:53:42:d5:e5:23.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added &#39;ec2-123-123-123-231.compute-1.amazonaws.com,123.123.123.231&#39; (RSA) to the list of known hosts.
Last login: Wed May 12 00:13:51 2010 from 192.168.1.1

The programs included with the Ubuntu system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/\*/copyright.

Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
applicable law.

To access official Ubuntu documentation, please visit:
http://help.ubuntu.com/

Created From:
Amazon EC2 Ubuntu 9.10 jaunty AMI built by Eric Hammond
http://alestic.com http://ec2ubuntu-group.notlong.com

StarCluster EC2 AMI created by Justin Riley (MIT)
url: http://web.mit.edu/stardev/cluster
email: star &#39;at&#39; mit &#39;dot&#39; edu
root@domU-12-31-38-00-A0-61:~#
</pre></div>
</div>
<p>This command is used frequently in the sections below to ensure that you&#8217;re logged into
the master node of a StarCluster on Amazon&#8217;s EC2 as root.</p>
</div>
<div class="section" id="logging-into-a-worker-node">
<h2>Logging into a worker node<a class="headerlink" href="#logging-into-a-worker-node" title="Permalink to this headline">¶</a></h2>
<p>You also have the option of logging into any particular worker node as root by using the
<strong>sshnode</strong> command. First, run &#8220;starcluster listclusters&#8221; to list the nodes:</p>
<div class="highlight-none"><div class="highlight"><pre>$ starcluster listclusters
StarCluster - (http://web.mit.edu/starcluster)
Software Tools for Academics and Researchers (STAR)
Please submit bug reports to starcluster@mit.edu

@sc-mycluster
master i-33333333 running ec2-123-23-23-22.compute-1.amazonaws.com
node001 i-99999999 running ec2-123-23-23-23.compute-1.amazonaws.com
node002 i-88888888 running ec2-123-23-23-24.compute-1.amazonaws.com
node003 i-77777777 running ec2-123-23-23-25.compute-1.amazonaws.com
...
</pre></div>
</div>
<p>Then use &#8220;starcluster sshnode mycluster&#8221; to login to a node:</p>
<div class="highlight-none"><div class="highlight"><pre>$ starcluster sshnode mycluster node001
StarCluster - (http://web.mit.edu/starcluster)
Software Tools for Academics and Researchers (STAR)
Please submit bug reports to starcluster@mit.edu

The authenticity of host &#39;ec2-123-123-123-232.compute-1.amazonaws.com (123.123.123.232)&#39; can&#39;t be established.
RSA key fingerprint is 86:23:b0:7e:23:c8:d1:02:4f:ba:22:53:42:d5:e5:23.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added &#39;ec2-123-123-123-232.compute-1.amazonaws.com,123.123.123.232&#39; (RSA) to the list of known hosts.
Last login: Wed May 12 00:13:51 2010 from 192.168.1.1

The programs included with the Ubuntu system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/\*/copyright.

Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
applicable law.

To access official Ubuntu documentation, please visit:
http://help.ubuntu.com/

Created From:
Amazon EC2 Ubuntu 9.04 jaunty AMI built by Eric Hammond
http://alestic.com http://ec2ubuntu-group.notlong.com

StarCluster EC2 AMI created by Justin Riley (MIT)
url: http://web.mit.edu/stardev/cluster
email: star &#39;at&#39; mit &#39;dot&#39; edu

0 packages can be updated.
0 updates are security updates.

root@domU-12-31-38-00-A2-44:~#
</pre></div>
</div>
</div>
<div class="section" id="verify-etc-hosts">
<h2>Verify /etc/hosts<a class="headerlink" href="#verify-etc-hosts" title="Permalink to this headline">¶</a></h2>
<p>Once StarCluster is up, the /etc/hosts file should look like:</p>
<div class="highlight-none"><div class="highlight"><pre>$ starcluster sshmaster mycluster
root@domU-12-31-38-00-A2-43:~# cat /etc/hosts
# Do not remove the following line or programs that require network functionality will fail
127.0.0.1 localhost.localdomain localhost
10.252.167.143 domU-12-31-38-00-A0-61.compute-1.internal domU-12-31-38-00-A0-61 master
10.252.165.173 domU-12-31-38-00-A2-43.compute-1.internal domU-12-31-38-00-A2-43 node001
</pre></div>
</div>
<p>As you can see, the head node is assigned an alias of &#8216;master&#8217; and each node after that is labeled node001, node002, etc.</p>
<p>In this example we have two nodes so only master and node001 are in /etc/hosts</p>
</div>
<div class="section" id="verify-passwordless-ssh">
<h2>Verify Passwordless SSH<a class="headerlink" href="#verify-passwordless-ssh" title="Permalink to this headline">¶</a></h2>
<p>StarCluster should have automatically setup passwordless ssh for both root and the CLUSTER_USER you specified.</p>
<p>To test this out, let&#8217;s login to the master node and attempt to run the hostname command via SSH on node001 without a password for both root and sgeadmin (ie CLUSTER_USER):</p>
<div class="highlight-none"><div class="highlight"><pre>$ starcluster sshmaster mycluster
root@domU-12-31-38-00-A0-61:~# ssh node001 hostname
domU-12-31-38-00-A2-43
root@domU-12-31-38-00-A0-61:~# su - sgeadmin
sgeadmin@domU-12-31-38-00-A0-61:~# ssh node001 hostname
domU-12-31-38-00-A2-43
sgeadmin@domU-12-31-38-00-A0-61:~# exit
root@domU-12-31-38-00-A0-61:~#
</pre></div>
</div>
</div>
<div class="section" id="verify-home-is-nfs-shared">
<h2>Verify /home is NFS Shared<a class="headerlink" href="#verify-home-is-nfs-shared" title="Permalink to this headline">¶</a></h2>
<p>The /home folder on all clusters launched by StarCluster should be NFS shared to each node. To check this, login to the master as root
and run the mount command on each node to verify that /home is mounted from the master:</p>
<div class="highlight-none"><div class="highlight"><pre>$ starcluster sshmaster mycluster
root@domU-12-31-38-00-A0-61:~# ssh node001 mount
/dev/sda1 on / type ext3 (rw)
none on /proc type proc (rw)
none on /sys type sysfs (rw)
/dev/sda2 on /mnt type ext3 (rw)
none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)
domU-12-31-38-00-A0-61.compute-1.internal:/home on /home type nfs (rw,user=root,nosuid,nodev,user,addr=10.215.42.81)
</pre></div>
</div>
<p>The last line in the output above indicates that /home is mounted from the master node over NFS. Running this for the rest of the nodes (e.g. node002, node003, etc)
should produce the same output.</p>
</div>
<div class="section" id="ensure-ebs-volumes-are-mounted-and-nfs-shared-optional">
<h2>Ensure EBS Volumes are Mounted and NFS shared (OPTIONAL)<a class="headerlink" href="#ensure-ebs-volumes-are-mounted-and-nfs-shared-optional" title="Permalink to this headline">¶</a></h2>
<p>If you chose to use EBS for persistent storage (recommended) you should check that it is
mounted and shared across the cluster via NFS at the location you specified in the config.
To do this we login to the master and run a few commands to ensure everything is working properly.
For this example we assume that a single 20GB volume has been attached to the cluster and that the volume
has <em>MOUNT_PATH=/home</em> in the config. If you&#8217;ve attached multiple EBS volumes to the cluster, you
should repeat these checks for each volume you specified in the config.</p>
<p>The first thing we want to do is to make sure the device was actually attached to the master
node as a device. To check that the device is attached on the master node, we login to the
master and use &#8220;fdisk -l&#8221; to look for our volume:</p>
<div class="highlight-none"><div class="highlight"><pre>$ starcluster sshmaster mycluster

root@domU-12-31-38-00-A0-61:~# fdisk -l

...

Disk /dev/sdz: 21.4 GB, 21474836480 bytes
255 heads, 63 sectors/track, 2610 cylinders
    Units = cylinders of 16065 * 512 = 8225280 bytes
    Disk identifier: 0x2a2a3cscg

    Device Boot Start End Blocks Id System
    /dev/sdz1 1 2610 20964793+ 83 Linux
</pre></div>
</div>
<p>From the output of fdisk above we see that there is indeed a 20GB device /dev/sdz with
partition /dev/sdz1 attached on the master node.</p>
<p>Next check the output of mount on the master node to ensure that the volume&#8217;s <em>PARTITION</em>
setting (which defaults to 1 if not specified) has been mounted to the volume&#8217;s <em>MOUNT_PATH</em>
setting specified in the config (/home for this example):</p>
<div class="highlight-none"><div class="highlight"><pre>root@domU-12-31-38-00-A0-61:~# mount
...
/dev/sdz1 on /home type ext3 (rw)
...
</pre></div>
</div>
<p>From the output of mount we see that the partition /dev/sdz1 has been mounted to /home
on the master node as we specified in the config.</p>
<p>Finally we check that the <em>MOUNT_PATH</em> specified in the config for this volume has been NFS
shared to each cluster node by running mount on each node and examining the output:</p>
<div class="highlight-none"><div class="highlight"><pre>$ starcluster sshmaster mycluster
root@domU-12-31-38-00-A0-61:~# ssh node001 mount
/dev/sda1 on / type ext3 (rw)
none on /proc type proc (rw)
none on /sys type sysfs (rw)
/dev/sda2 on /mnt type ext3 (rw)
none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)
domU-12-31-38-00-A0-61.compute-1.internal:/home on /home type nfs (rw,user=root,nosuid,nodev,user,addr=10.215.42.81)
root@domU-12-31-38-00-A0-61:~# ssh node002 mount
...
domU-12-31-38-00-A0-61.compute-1.internal:/home on /home type nfs (rw,user=root,nosuid,nodev,user,addr=10.215.42.81)
...
</pre></div>
</div>
<p>The last line in the output above indicates that <em>MOUNT_PATH</em> (/home for this example) is mounted
on each worker node from the master node via NFS.  Running this for the rest of the nodes
(e.g. node002, node003, etc) should produce the same output.</p>
</div>
<div class="section" id="verify-scratch-space">
<h2>Verify scratch space<a class="headerlink" href="#verify-scratch-space" title="Permalink to this headline">¶</a></h2>
<p>Each node should be set up with approximately 140GB or more of local scratch space for writing
temporary files instead of storing temporary files on NFS. The location of the scratch
space is /scratch/CLUSTER_USER. So, for this example the local scratch for
CLUSTER_USER=sgeadmin is /scratch/sgeadmin.</p>
<p>To verify this, login to the master and run ls -l /scratch.</p>
<div class="highlight-none"><div class="highlight"><pre>$ starcluster sshmaster mycluster
root@domU-12-31-38-00-A0-61:/# ls -l /scratch/
total 0
lrwxrwxrwx 1 root root 13 2009-09-09 14:34 sgeadmin -&gt; /mnt/sgeadmin
</pre></div>
</div>
<p>From the output above we see that /scratch/sgeadmin has been symbolically linked
to /mnt/sgeadmin</p>
<p>Next we run the df command to verify that at least ~140GB is available on /mnt (and thus
/mnt/sgeadmin)</p>
<div class="highlight-none"><div class="highlight"><pre>root@domU-12-31-38-00-A0-61:/# df -h
Filesystem Size Used Avail Use% Mounted on
...
/dev/sda2 147G 188M 140G 1% /mnt
...
sgeadmin@domU-12-31-38-00-A0-61:~$
</pre></div>
</div>
</div>
<div class="section" id="compile-and-run-a-hello-world-openmpi-program">
<h2>Compile and run a &#8220;Hello World&#8221; OpenMPI program<a class="headerlink" href="#compile-and-run-a-hello-world-openmpi-program" title="Permalink to this headline">¶</a></h2>
<p>Below is a simple Hello World program in MPI (retrieved from here)</p>
<div class="highlight-c"><div class="highlight"><pre><span class="cp">#include &lt;stdio.h&gt; </span><span class="cm">/* printf and BUFSIZ defined there */</span><span class="cp"></span>
<span class="cp">#include &lt;stdlib.h&gt; </span><span class="cm">/* exit defined there */</span><span class="cp"></span>
<span class="cp">#include &lt;mpi.h&gt; </span><span class="cm">/* all MPI-2 functions defined there */</span><span class="cp"></span>

<span class="kt">int</span> <span class="n">main</span><span class="p">(</span><span class="n">argc</span><span class="p">,</span> <span class="n">argv</span><span class="p">)</span>
        <span class="kt">int</span> <span class="n">argc</span><span class="p">;</span>
        <span class="kt">char</span> <span class="o">*</span><span class="n">argv</span><span class="p">[];</span>
        <span class="p">{</span>
        <span class="kt">int</span> <span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">length</span><span class="p">;</span>
        <span class="kt">char</span> <span class="n">name</span><span class="p">[</span><span class="n">BUFSIZ</span><span class="p">];</span>

        <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
        <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rank</span><span class="p">);</span>
        <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">size</span><span class="p">);</span>
        <span class="n">MPI_Get_processor_name</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">length</span><span class="p">);</span>

        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;%s: hello world from process %d of %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>

        <span class="n">MPI_Finalize</span><span class="p">();</span>

        <span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Save this code to a file called helloworldmpi.c in /home/sgeadmin. You can then
compile and run the code across the cluster like so:</p>
<div class="highlight-none"><div class="highlight"><pre>$ starcluster sshmaster mycluster
root@domU-12-31-38-00-A0-61:~# su - sgeadmin
sgeadmin@domU-12-31-38-00-A0-61:~$ mpicc helloworldmpi.c -o helloworldmpi
sgeadmin@domU-12-31-38-00-A0-61:~$ mpirun -n 2 -host master,node001 ./helloworldmpi
domU-12-31-38-00-A0-61: hello world from process 0 of 2
domU-12-31-38-00-A2-43: hello world from process 1 of 2
sgeadmin@domU-12-31-38-00-A0-61:~$
</pre></div>
</div>
<p>Obviously if you have more nodes, the -host mater,node001 list specified will
need to be extended. You can also create a hostfile instead of listing each
node for OpenMPI to use that looks like:</p>
<div class="highlight-none"><div class="highlight"><pre>sgeadmin@domU-12-31-38-00-A0-61:~$ cat /home/sgeadmin/hostfile
master
node001
</pre></div>
</div>
<p>After creating this hostfile, you can now call mpirun with less options:</p>
<div class="highlight-none"><div class="highlight"><pre>sgeadmin@domU-12-31-38-00-A0-61:~$ mpirun -n 2 -hostfile /home/sgeadmin/hostfile ./helloworldmpi
domU-12-31-38-00-A0-61: hello world from process 0 of 2
domU-12-31-38-00-A2-43: hello world from process 1 of 2
sgeadmin@domU-12-31-38-00-A0-61:~$
</pre></div>
</div>
</div>
<div class="section" id="sun-grid-engine-sge-quickstart">
<h2>Sun Grid Engine (SGE) QuickStart<a class="headerlink" href="#sun-grid-engine-sge-quickstart" title="Permalink to this headline">¶</a></h2>
<p>Submit a Simple Job through Sun Grid Engine
Submit a job that runs hostname on a single node to Sun Grid Engine</p>
<div class="highlight-none"><div class="highlight"><pre>sgeadmin@domU-12-31-38-00-A0-61:~$ qsub -V -b y -cwd hostname
Your job 1 (&quot;hostname&quot;) has been submitted
</pre></div>
</div>
<p>The -V option to qsub states that the job should have the same environment
variables as the shell executing qsub (recommended)</p>
<p>The -b option to qsub states that the command being executed could be a single
binary executable or a bash script. In this case the command &#8216;hostname&#8217; is a
single binary.</p>
<p>The -cwd option to qsub tells Sun Grid Engine that the job should be executed in
the same directory that qsub was called.</p>
<p>The last argument to qsub is the command to be executed (in this case &#8216;hostname&#8217;)</p>
</div>
<div class="section" id="monitoring-jobs-in-the-queue">
<h2>Monitoring Jobs in the Queue<a class="headerlink" href="#monitoring-jobs-in-the-queue" title="Permalink to this headline">¶</a></h2>
<p>Now that our job has been submitted, let&#8217;s take a look at the job&#8217;s status in
the queue using the command &#8216;qstat&#8217;:</p>
<div class="highlight-none"><div class="highlight"><pre>sgeadmin@domU-12-31-38-00-A0-61:~$ qstat
job-ID prior name user state submit/start at queue slots ja-task-ID
-----------------------------------------------------------------------------------------
1 0.00000 hostname sgeadmin qw 09/09/2009 14:58:00 1
sgeadmin@domU-12-31-38-00-A0-61:~$
</pre></div>
</div>
<p>From this output, we can see that the job is in the <em>qw</em> state which stands for
&#8216;queued and waiting&#8217;. After a few seconds, the job will transition into a <em>r</em>,
or &#8216;running&#8217;, state.</p>
<div class="highlight-none"><div class="highlight"><pre>sgeadmin@domU-12-31-38-00-A0-61:~$ qstat
job-ID  prior   name       user         state submit/start at     queue  slots ja-task-ID
-----------------------------------------------------------------------------------------
1 0.00000 hostname   sgeadmin     r     09/09/2009 14:58:14                1
sgeadmin@domU-12-31-38-00-A0-61:~$
</pre></div>
</div>
<p>Once the job has finished, the job will be removed from the queue and will no
longer appear in the output of qstat:</p>
<div class="highlight-none"><div class="highlight"><pre>sgeadmin@domU-12-31-38-00-A0-61:~$ qstat
sgeadmin@domU-12-31-38-00-A0-61:~$
</pre></div>
</div>
</div>
<div class="section" id="viewing-a-job-s-output">
<h2>Viewing a Job&#8217;s Output<a class="headerlink" href="#viewing-a-job-s-output" title="Permalink to this headline">¶</a></h2>
<p>Sun Grid Engine creates stdout and stderr files in the job&#8217;s working directory
for each job executed. If any additional files are created during a job&#8217;s execution,
they will also be located in the job&#8217;s working directory unless explicitly saved
elsewhere.</p>
<p>The job&#8217;s stdout and stderr files are named after the job with the extension ending
in the job&#8217;s number.</p>
<p>For the simple job submitted above we have:</p>
<div class="highlight-none"><div class="highlight"><pre>sgeadmin@domU-12-31-38-00-A0-61:~$ ls hostname.*
hostname.e1 hostname.o1
sgeadmin@domU-12-31-38-00-A0-61:~$ cat hostname.o1
domU-12-31-38-00-A2-43
sgeadmin@domU-12-31-38-00-A0-61:~$ cat hostname.e1
sgeadmin@domU-12-31-38-00-A0-61:~$
</pre></div>
</div>
<p>Notice that Sun Grid Engine automatically named the job &#8216;hostname&#8217; and created two
output files: hostname.e1 and hostname.o1. The &#8216;e&#8217; stands for stderr and the &#8216;o&#8217; for stdout.
The 1 at the end of the files&#8217; extension is the job number. So if the job had been named
&#8216;my_new_job&#8217; and was job #23 submitted, the output files would look like:</p>
<div class="highlight-none"><div class="highlight"><pre>my_new_job.e23 my_new_job.o23
</pre></div>
</div>
</div>
<div class="section" id="monitoring-cluster-usage">
<h2>Monitoring Cluster Usage<a class="headerlink" href="#monitoring-cluster-usage" title="Permalink to this headline">¶</a></h2>
<p>After a while you may be curious to view the load on Sun Grid Engine. To do this,
we use the qhost command:</p>
<div class="highlight-none"><div class="highlight"><pre>sgeadmin@domU-12-31-38-00-A0-61:~$ qhost
HOSTNAME ARCH NCPU LOAD MEMTOT MEMUSE SWAPTO SWAPUS
-------------------------------------------------------------------------------
global - - - - - - -
domU-12-31-38-00-A0-61 lx24-x86 1 0.00 1.7G 62.7M 896.0M 0.0
domU-12-31-38-00-A2-43 lx24-x86 1 0.00 1.7G 47.8M 896.0M 0.0
</pre></div>
</div>
<p>The output shows the architecture (ARCH), number of cpus (NCPU), the current load (LOAD),
total memory (MEMTOT), and currently used memory (MEMUSE) and swap space (SWAPTO) for each node.</p>
<p>You can also view the average load (load_avg) per node using the &#8216;-f&#8217; option to qstat:</p>
<div class="highlight-none"><div class="highlight"><pre>sgeadmin@domU-12-31-38-00-A0-61:~$ qstat -f
queuename qtype resv/used/tot. load_avg arch states
---------------------------------------------------------------------------------
all.q@domU-12-31-38-00-A0-61.c BIP 0/0/1 0.00 lx24-x86
---------------------------------------------------------------------------------
all.q@domU-12-31-38-00-A2-43.c BIP 0/0/1 0.00 lx24-x86
sgeadmin@domU-12-31-38-00-A0-61:~$
</pre></div>
</div>
</div>
<div class="section" id="creating-a-job-script">
<h2>Creating a Job Script<a class="headerlink" href="#creating-a-job-script" title="Permalink to this headline">¶</a></h2>
<p>In the &#8216;Submit a Simple Job&#8217; section we submitted a single command &#8216;hostname&#8217;.
This is useful for simple jobs but for more complex jobs where we need to incorporate
some logic we can use a so-called &#8216;job script&#8217;. A &#8216;job script&#8217; is essentially a bash
script that contains some logic and executes any number of external programs/scripts:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="c">#!/bin/bash</span>
<span class="nb">echo</span> <span class="s2">&quot;hello from job script!&quot;</span>
<span class="nb">echo</span> <span class="s2">&quot;the date is&quot;</span> <span class="sb">`</span>date<span class="sb">`</span>
<span class="nb">echo</span> <span class="s2">&quot;here&#39;s /etc/hosts contents:&quot;</span>
cat /etc/hosts
<span class="nb">echo</span> <span class="s2">&quot;finishing job :D&quot;</span>
</pre></div>
</div>
<p>As you can see, this script simply executes a few commands (such as echo, date, cat, etc)
and exits. Anything printed to the screen will be put in the job&#8217;s stdout file by Sun Grid Engine.</p>
<p>Since this is just a bash script, you can put any form of logic necessary in the job script
(ie if statements, while loops, for loops, etc) and you may call any number of external programs
needed to complete the job.</p>
<p>Let&#8217;s see how you run this new job script. Save the script above to /home/sgeadmin/jobscript.sh
on your StarCluster and execute the following as the sgeadmin user:</p>
<div class="highlight-none"><div class="highlight"><pre>sgeadmin@domU-12-31-38-00-A0-61:~$ qsub -V jobscript.sh
Your job 6 (&quot;jobscript.sh&quot;) has been submitted
</pre></div>
</div>
<p>Now that the job has been submitted, let&#8217;s call qstat periodically until the job has finished
since this job should only take a second to run once it&#8217;s executed:</p>
<div class="highlight-none"><div class="highlight"><pre>sgeadmin@domU-12-31-38-00-A0-61:~$ qstat
job-ID prior name user state submit/start at queue slots ja-task-ID
-----------------------------------------------------------------------------------------
6 0.00000 jobscript. sgeadmin qw 09/09/2009 16:18:43 1

sgeadmin@domU-12-31-38-00-A0-61:~$ qstat
job-ID prior name user state submit/start at queue slots ja-task-ID
-----------------------------------------------------------------------------------------
6 0.00000 jobscript. sgeadmin qw 09/09/2009 16:18:43 1

sgeadmin@domU-12-31-38-00-A0-61:~$ qstat
job-ID prior name user state submit/start at queue slots ja-task-ID
-----------------------------------------------------------------------------------------
6 0.00000 jobscript. sgeadmin qw 09/09/2009 16:18:43 1

sgeadmin@domU-12-31-38-00-A0-61:~$ qstat
job-ID prior name user state submit/start at queue slots ja-task-ID
-----------------------------------------------------------------------------------------
6 0.00000 jobscript. sgeadmin qw 09/09/2009 16:18:43 1

sgeadmin@domU-12-31-38-00-A0-61:~$ qstat
job-ID prior name user state submit/start at queue slots ja-task-ID
-----------------------------------------------------------------------------------------
6 0.55500 jobscript. sgeadmin r 09/09/2009 16:18:57 all.q@domU-12-31-38-00-A2-43.c 1

sgeadmin@domU-12-31-38-00-A0-61:~$ qstat
sgeadmin@domU-12-31-38-00-A0-61:~$
</pre></div>
</div>
<p>Now that the job is finished, let&#8217;s take a look at the output files:</p>
<div class="highlight-none"><div class="highlight"><pre>sgeadmin@domU-12-31-38-00-A0-61:~$ ls jobscript.sh*
jobscript.sh jobscript.sh.e6 jobscript.sh.o6
sgeadmin@domU-12-31-38-00-A0-61:~$ cat jobscript.sh.o6
hello from job script!
the date is Wed Sep 9 16:18:57 UTC 2009
here&#39;s /etc/hosts contents:
# Do not remove the following line or programs that require network functionality will fail
127.0.0.1 localhost.localdomain localhost
10.252.167.143 domU-12-31-38-00-A0-61.compute-1.internal domU-12-31-38-00-A0-61 master
10.252.165.173 domU-12-31-38-00-A2-43.compute-1.internal domU-12-31-38-00-A2-43 node001
finishing job :D
sgeadmin@domU-12-31-38-00-A0-61:~$ cat jobscript.sh.e6
sgeadmin@domU-12-31-38-00-A0-61:~$
</pre></div>
</div>
<p>We see from looking at the output that the stdout file contains the output of the
echo,date, and cat statements in the job script and that the stderr file is blank
meaning there were no errors during the job&#8217;s execution. Had something failed, such
as a command not found error for example, these errors would have appeared in the
stderr file.</p>
</div>
<div class="section" id="deleting-a-job-from-the-queue">
<h2>Deleting a Job from the Queue<a class="headerlink" href="#deleting-a-job-from-the-queue" title="Permalink to this headline">¶</a></h2>
<p>What if a job is stuck in the queue, is taking too long to run, or was simply
started with incorrect parameters? You can delete a job from the queue using the &#8216;qdel&#8217;
command in Sun Grid Engine. Below we launch a simple &#8216;sleep&#8217; job that sleeps for 10
seconds so that we can kill it using &#8216;qdel&#8217;:</p>
<div class="highlight-none"><div class="highlight"><pre>sgeadmin@domU-12-31-38-00-A0-61:~$ qsub -b y -cwd sleep 10
Your job 3 (&quot;sleep&quot;) has been submitted
sgeadmin@domU-12-31-38-00-A0-61:~$ qdel 3
sgeadmin has registered the job 3 for deletion
</pre></div>
</div>
<p>After running qdel you&#8217;ll notice the job is gone from the queue:</p>
<div class="highlight-none"><div class="highlight"><pre>sgeadmin@domU-12-31-38-00-A0-61:~$ qstat
sgeadmin@domU-12-31-38-00-A0-61:~$
</pre></div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="create_new_ami.html" title="Creating a New AMI From the StarCluster AMI"
             >next</a> |</li>
        <li class="right" >
          <a href="launch.html" title="Launching a StarCluster on Amazon EC2"
             >previous</a> |</li>
        <li><a href="index.html">home</a>|&nbsp;</li>
        <li><a href="search.html">search</a>|&nbsp;</li>
       <li><a href="contents.html">documentation </a> &raquo;</li>
 
      </ul>
    </div>
    <div class="footer">
      &copy; Copyright 2010, Software Tools for Academics and Researchers.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 0.6.5.
    </div>
  </body>
</html>