<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Topics and Transformations &mdash; gensim documentation</title>
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '#',
        VERSION:     '0.4.7',
        COLLAPSE_MODINDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="gensim documentation" href="index.html" />
    <link rel="up" title="Tutorial" href="tutorial.html" />
    <link rel="next" title="Similarity Queries" href="tut3.html" />
    <link rel="prev" title="Corpora and Vector Spaces" href="tut1.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="modindex.html" title="Global Module Index"
             accesskey="M">modules</a> |</li>
        <li class="right" >
          <a href="tut3.html" title="Similarity Queries"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="tut1.html" title="Corpora and Vector Spaces"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">gensim documentation</a> &raquo;</li>
          <li><a href="tutorial.html" accesskey="U">Tutorial</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="topics-and-transformations">
<span id="tut2"></span><h1>Topics and Transformations<a class="headerlink" href="#topics-and-transformations" title="Permalink to this headline">¶</a></h1>
<p>Don&#8217;t forget to set</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">logging</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logging</span><span class="o">.</span><span class="n">root</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span> <span class="c"># will suppress DEBUG level events</span>
</pre></div>
</div>
<p>if you want to see logging events.</p>
<div class="section" id="transformation-interface">
<h2>Transformation interface<a class="headerlink" href="#transformation-interface" title="Permalink to this headline">¶</a></h2>
<p>In the previous tutorial on <a class="reference external" href="tut1.html"><em>Corpora and Vector Spaces</em></a>, we created a corpus of documents represented
as a stream of vectors. To continue, let&#8217;s fire up gensim and use that corpus:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">gensim</span> <span class="kn">import</span> <span class="n">corpora</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">similarities</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dictionary</span> <span class="o">=</span> <span class="n">corpora</span><span class="o">.</span><span class="n">Dictionary</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&#39;/tmp/dictionary.pkl&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">corpus</span> <span class="o">=</span> <span class="n">corpora</span><span class="o">.</span><span class="n">MmCorpus</span><span class="p">(</span><span class="s">&#39;/tmp/deerwester.mm&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">corpus</span>
<span class="go">MmCorpus(9 documents, 12 features, 28 non-zero entries)</span>
</pre></div>
</div>
<p>In this tutorial, we will show how to transform documents from one vector representation
into another. This process serves two goals:</p>
<ol class="arabic simple">
<li>To bring out hidden structure in the corpus, discover relationships between
the original features and use them to describe the documents in a new and
(hopefully) more realistic way.</li>
<li>To make the document representation more compact. This both improves efficiency
(new representation consumes less resources) and efficacy (marginal data
trends are ignored, so that transformations can be thought of as noise-reduction).</li>
</ol>
<p>The transformations are standard Python objects, typically initialized by means of
a <em>training corpus</em>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">tfidf</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">TfidfModel</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</pre></div>
</div>
<p>We used our old corpus to initialize (train) the transformation model. In case of TfIdf, the
&#8220;training&#8221; consists of going through the corpus once and computing document frequencies
of all its features.</p>
<p>From now on, <tt class="docutils literal"><span class="pre">tfidf</span></tt> is treated as a read-only object that can be used to convert
any vector from the old representation (bag-of-words counts) to the new representation
(TfIdf):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">doc_bow</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">tfidf</span><span class="p">[</span><span class="n">doc_bow</span><span class="p">]</span>
<span class="go">[(0, 0.70710678), (1, 0.70710678)]</span>
</pre></div>
</div>
<p>To apply a transformation to the whole corpus:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">corpus_tfidf</span> <span class="o">=</span> <span class="n">tfidf</span><span class="p">[</span><span class="n">corpus</span><span class="p">]</span>
</pre></div>
</div>
<p>This creates a standard corpus (stream of documents), which can be iterated over,
saved to disk etc.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Calling <tt class="docutils literal"><span class="pre">transformation[corpus]</span></tt> only creates a wrapper around the old <tt class="docutils literal"><span class="pre">corpus</span></tt>
document stream &#8211; actual conversions are done on-the-fly, during document iteration.
Conversion at the time of calling <tt class="docutils literal"><span class="pre">corpus2</span> <span class="pre">=</span> <span class="pre">transformation[corpus]</span></tt> would mean
storing the result in memory, which contradicts gensim&#8217;s objective of memory-indepedence.
If you will be iterating over the transformed <tt class="docutils literal"><span class="pre">corpus2</span></tt> multiple times, and the
transformation is costly, <a class="reference external" href="tut1.html#corpus-formats"><em>store the resulting corpus to disk first</em></a> and continue
using that.</p>
</div>
<p>Transformations can also be serialized, one on top of another, in a sort of chain:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">lsi</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">LsiModel</span><span class="p">(</span><span class="n">corpus_tfidf</span><span class="p">,</span> <span class="n">id2word</span> <span class="o">=</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">id2token</span><span class="p">,</span> <span class="n">numTopics</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span> <span class="c"># initialize LSI transformation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">corpus_lsi</span> <span class="o">=</span> <span class="n">lsi</span><span class="p">[</span><span class="n">corpus_tfidf</span><span class="p">]</span> <span class="c"># create a double wrapper over the original corpus: bow-&gt;tfidf-&gt;fold-in-lsi</span>
</pre></div>
</div>
<p>Here we transformed our Tf-Idf corpus via <a class="reference external" href="http://en.wikipedia.org/wiki/Latent_semantic_indexing">Latent Semantic Indexing</a>
into a latent 2-D space. Now you&#8217;re probably wondering what these two latent
dimensions stand for, so let&#8217;s inspect with <tt class="xref docutils literal"><span class="pre">models.LsiModel.printTopics()</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">topicNo</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">lsi</span><span class="o">.</span><span class="n">numTopics</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">print</span> <span class="s">&#39;topic </span><span class="si">%i</span><span class="s">: </span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">topicNo</span><span class="p">,</span> <span class="n">lsi</span><span class="o">.</span><span class="n">printTopic</span><span class="p">(</span><span class="n">topicNo</span><span class="p">))</span>
<span class="go">topic 0: -0.703 * &quot;trees&quot; + -0.538 * &quot;graph&quot; + -0.402 * &quot;minors&quot; + -0.187 * &quot;survey&quot; + -0.061 * &quot;system&quot; + -0.060 * &quot;time&quot; + -0.060 * &quot;response&quot; + -0.058 * &quot;user&quot; + -0.049 * &quot;computer&quot; + -0.035 * &quot;interface&quot; + -0.035 * &quot;eps&quot; + -0.030 * &quot;human&quot;</span>
<span class="go">topic 1: 0.460 * &quot;system&quot; + 0.373 * &quot;user&quot; + 0.332 * &quot;eps&quot; + 0.328 * &quot;interface&quot; + 0.320 * &quot;time&quot; + 0.320 * &quot;response&quot; + 0.293 * &quot;computer&quot; + 0.280 * &quot;human&quot; + 0.171 * &quot;survey&quot; + -0.161 * &quot;trees&quot; + -0.076 * &quot;graph&quot; + -0.029 * &quot;minors&quot;</span>
</pre></div>
</div>
<p>It appears that according to LSI, &#8220;trees&#8221;, &#8220;graphs&#8221; and &#8220;minors&#8221; are all related
words (and contribute the most to the direction of the first topic), while the
second topic practically concerns itself with all the other words. As expected,
the first five documents are more strongly related to the second topic and the
remaining four documents to the first topic:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">corpus_lsi</span><span class="p">:</span> <span class="c"># both bow-&gt;tfidf and tfidf-&gt;lsi transformations are actually executed here</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">print</span> <span class="n">doc</span>
<span class="go">[(0, -0.066), (1, 0.520)] # &quot;Human machine interface for lab abc computer applications&quot;</span>
<span class="go">[(0, -0.197), (1, 0.761)] # &quot;A survey of user opinion of computer system response time&quot;</span>
<span class="go">[(0, -0.090), (1, 0.724)] # &quot;The EPS user interface management system&quot;</span>
<span class="go">[(0, -0.076), (1, 0.632)] # &quot;System and human system engineering testing of EPS&quot;</span>
<span class="go">[(0, -0.102), (1, 0.574)] # &quot;Relation of user perceived response time to error measurement&quot;</span>
<span class="go">[(0, -0.703), (1, -0.161)] # &quot;The generation of random binary unordered trees&quot;</span>
<span class="go">[(0, -0.877), (1, -0.168)] # &quot;The intersection graph of paths in trees&quot;</span>
<span class="go">[(0, -0.910), (1, -0.141)] # &quot;Graph minors IV Widths of trees and well quasi ordering&quot;</span>
<span class="go">[(0, -0.617), (1, 0.054)] # &quot;Graph minors A survey&quot;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Transformations are initialized to convert between two specific vector
spaces. Failure to use the same input feature space (such as applying a different string
preprocessing, or using bag-of-words vectors where TfIdf vectors are expected)
will result in feature mismatch during the transformation, and consequently in
either garbage output and/or runtime exceptions.</p>
</div>
<p>Model persistency is handled via the <tt class="xref docutils literal"><span class="pre">save()</span></tt> and <tt class="xref docutils literal"><span class="pre">load()</span></tt> functions:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">lsi</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">&#39;/tmp/model.lsi&#39;</span><span class="p">)</span> <span class="c"># same for tfidf, lda, ...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lsi</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">LsiModel</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&#39;/tmp/model.lsi&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The next question might be: just how exactly similar are those documents to each other?
Is there a way to formalize the similarity, so that for a given document, we can
order some other set of documents according to their similarity? Similarity queries
are covered in the <a class="reference external" href="tut3.html"><em>next tutorial</em></a>.</p>
</div>
<div class="section" id="available-transformations">
<h2>Available transformations<a class="headerlink" href="#available-transformations" title="Permalink to this headline">¶</a></h2>
<p>Gensim implements several popular Vector Space Model algorithms:</p>
<ul class="simple">
<li><a class="reference external" href="http://en.wikipedia.org/wiki/Latent_semantic_indexing">Latent Semantic Indexing, LSI, LSA</a>
transforms documents from either bag-of-words or (preferrably) TfIdf-weighted space into
a latent space of a lower dimensionality. For the toy corpus above we used only
2 latent dimensions, but on real corpora, target dimensionality of 200&#8211;500 is recommended
as a &#8220;golden standard&#8221; <a class="footnote-reference" href="#id4" id="id1">[1]</a>.</li>
<li><a class="reference external" href="www.cis.hut.fi/ella/publications/randproj_kdd.pdf">Random Projections, RP</a> aim to
reduce vector space dimensionality. This is a very efficient (both memory- and
CPU-friendly) approach to approximating TfIdf through randomness. Recommended
target dimensionality is again in the hundreds/thousands, depending on your dataset.</li>
<li><a class="reference external" href="http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Latent Dirichlet Allocation, LDA</a>
is yet another transformation, from bag-of-words counts into a topic space of low
dimensionality (a few hundreds). LDA is <strong>much</strong> slower than the other algorithms,
and we are currently looking into ways of making it faster (see eg. <a class="footnote-reference" href="#id5" id="id2">[2]</a>). If you&#8217;d like
to contribute, <a class="reference external" href="mailto:radimrehurek&#37;&#52;&#48;seznam&#46;cz">let us know</a>!</li>
</ul>
<p>Adding new <abbr title="Vector Space Model">VSM</abbr> transformations (such as different weighting schemes) is rather trivial;
see the <a class="reference external" href="apiref.html"><em>API reference</em></a> or directly the Python code for more info and examples.</p>
<p>It is probably worth repeating that these are all unique, <strong>incremental</strong> implementations,
which do not require the whole training corpus to be present in main memory at once.
With memory taken care of, we are now investigating available lightweight Python
frameworks for distributed computing, to improve CPU efficiency, too.
If you feel you could contribute, please <a class="reference external" href="mailto:radimrehurek&#37;&#52;&#48;seznam&#46;cz">let us know</a>!</p>
<hr class="docutils" />
<table class="docutils footnote" frame="void" id="id4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>Bradford, R.B., 2008. An empirical study of required dimensionality for large-scale latent semantic indexing applications.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[2]</a></td><td>Asuncion, A., 2009. On Smoothing and Inference for Topic Models.</td></tr>
</tbody>
</table>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <h3><a href="index.html">Table Of Contents</a></h3>
            <ul>
<li><a class="reference external" href="#">Topics and Transformations</a><ul>
<li><a class="reference external" href="#transformation-interface">Transformation interface</a></li>
<li><a class="reference external" href="#available-transformations">Available transformations</a></li>
</ul>
</li>
</ul>

            <h4>Previous topic</h4>
            <p class="topless"><a href="tut1.html"
                                  title="previous chapter">Corpora and Vector Spaces</a></p>
            <h4>Next topic</h4>
            <p class="topless"><a href="tut3.html"
                                  title="next chapter">Similarity Queries</a></p>
          <div id="searchbox" style="display: none">
            <h3>Quick search</h3>
              <form class="search" action="search.html" method="get">
                <input type="text" name="q" size="18" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
              </form>
              <p class="searchtip" style="font-size: 90%">
              Enter search terms or a module, class or function name.
              </p>
          </div>
          <script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="modindex.html" title="Global Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="tut3.html" title="Similarity Queries"
             >next</a> |</li>
        <li class="right" >
          <a href="tut1.html" title="Corpora and Vector Spaces"
             >previous</a> |</li>
        <li><a href="index.html">gensim documentation</a> &raquo;</li>
          <li><a href="tutorial.html" >Tutorial</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
      &copy; Copyright 2010, Radim Řehůřek.
    </div>
  </body>
</html>